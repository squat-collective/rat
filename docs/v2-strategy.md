# RAT v2 â€” Strategic Direction

> Captured: Feb 2026
> Status: **Discussion / Pre-planning**
> v1: Python (FastAPI) â€” ship and learn
> v2: Go platform rewrite + Python runner containers

---

## Vision

**Community Edition** = the SQLite of data platforms. `docker compose up`, you're running.
No login, no config, no accounts. One user, full power. Everything in the browser.

**Pro Edition** = when you need teams, security, sharing, cloud. Delivered as closed-source
container plugins that snap into Community's extension points.

**Development model**: No local dev. No CLI. The web portal IS the IDE. Users write code
in the browser editor (backed by S3), run pipelines, test quality, view lineage â€” all from
the UI. CI/CD is built into the platform (run â†’ test â†’ promote), not external tools.

---

## Data Model: Namespaces + Per-Object Ownership

**Decision**: Namespace-based model with per-object ownership and sharing.

### Why Namespaces

- **No naming collisions** â€” two teams can both have `gold.revenue` in different namespaces
- **"Owner shares anything"** â€” any pipeline/table owner can share with anyone
- **Global discovery** â€” one catalog, one search, see everything you have access to
- **Global lineage** â€” full platform DAG across namespaces
- **Community simplicity** â€” one implicit namespace, invisible to user

### Namespace Model

```
namespace.layer.table_name

Community:  (implicit default namespace â€” user just sees layer.table)
  bronze.raw_orders
  silver.orders
  gold.revenue

Pro:  (explicit namespaces â€” like GitHub orgs)
  ecommerce.bronze.raw_orders        owner: alice
  ecommerce.silver.orders            owner: alice   shared_with: [bob:read]
  ecommerce.gold.revenue             owner: alice   shared_with: [bob:read, charlie:read]
  marketing.bronze.campaigns         owner: bob
  marketing.gold.attribution         owner: bob     shared_with: [alice:read]
```

- **Ownership**: every table and pipeline has an owner (the creator). Transferable.
- **Sharing**: owner grants per-object access â€” `SHARE gold.revenue WITH bob READ`
- **Projects** (soft grouping, Pro): organize tables/pipelines into logical groups. A table can belong to multiple projects. No isolation, just organization.
- **Namespaces** (Pro): organizational boundaries that prevent naming collisions. Users can belong to multiple namespaces. Cross-namespace sharing is natural.

### S3 Layout

```
s3://rat/
â”œâ”€â”€ {namespace}/                        # "default" for Community
â”‚   â”œâ”€â”€ pipelines/
â”‚   â”‚   â”œâ”€â”€ bronze/{pipeline_name}/
â”‚   â”‚   â”‚   â”œâ”€â”€ pipeline.sql
â”‚   â”‚   â”‚   â”œâ”€â”€ config.yaml
â”‚   â”‚   â”‚   â”œâ”€â”€ pipeline.meta.yaml
â”‚   â”‚   â”‚   â””â”€â”€ tests/quality/*.sql
â”‚   â”‚   â”œâ”€â”€ silver/{pipeline_name}/...
â”‚   â”‚   â””â”€â”€ gold/{pipeline_name}/...
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”œâ”€â”€ bronze/{table_name}/        # Iceberg table data
â”‚   â”‚   â”œâ”€â”€ silver/{table_name}/...
â”‚   â”‚   â””â”€â”€ gold/{table_name}/...
â”‚   â””â”€â”€ docs/
â”‚       â””â”€â”€ README.md
â””â”€â”€ ...                                 # other namespaces (Pro)
```

### Nessie Catalog Layout

- **Main branch**: All production tables. Flat namespace: `{namespace}.{layer}.{table_name}`
- **Dev branches**: For experimentation / preview. Users branch off main, test changes, merge back.
- Namespaces are NOT branches â€” they're part of the table path on `main`.

### How It Maps to Editions

| | Community | Pro |
|---|---|---|
| **Namespace** | One implicit (`default`), invisible | Multiple, user-managed (like GitHub orgs) |
| **Naming** | `layer.table` | `namespace.layer.table` |
| **Sharing** | Not needed (single user) | Owner shares anything with anyone |
| **Projects** | Not needed | Soft grouping for organization |
| **Discovery** | Browse your layers | Search/browse all tables you can access |
| **Permissions** | Allow-all | Per-object ACLs via plugin |

---

## Architecture: Microservices (Max Split)

All services are independent containers, orchestrated via `docker compose up`.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    docker compose up                              â”‚
â”‚                                                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  ratd    â”‚  â”‚  ratq    â”‚  â”‚  runner   â”‚  â”‚  portal          â”‚ â”‚
â”‚  â”‚  (Go)    â”‚  â”‚  (Python)â”‚  â”‚  (Python) â”‚  â”‚  (Next.js)       â”‚ â”‚
â”‚  â”‚          â”‚  â”‚          â”‚  â”‚           â”‚  â”‚                  â”‚ â”‚
â”‚  â”‚  API     â”‚  â”‚  DuckDB  â”‚  â”‚  Pipeline â”‚  â”‚  Web IDE         â”‚ â”‚
â”‚  â”‚  Auth    â”‚â—„â”€â”¤  Queries â”‚  â”‚  Executionâ”‚  â”‚  Editor          â”‚ â”‚
â”‚  â”‚  Sched   â”‚  â”‚  Read-   â”‚  â”‚  DuckDB   â”‚  â”‚  Query           â”‚ â”‚
â”‚  â”‚  Plugins â”‚  â”‚  only    â”‚  â”‚  PyArrow  â”‚  â”‚  Pipelines       â”‚ â”‚
â”‚  â”‚  Catalog â”‚  â”‚          â”‚  â”‚  Iceberg  â”‚  â”‚  DAG / Explorer  â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚       â”‚                                                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                       â”‚
â”‚  â”‚ postgres â”‚  â”‚  minio   â”‚  â”‚  nessie   â”‚                       â”‚
â”‚  â”‚ (state)  â”‚  â”‚  (S3)    â”‚  â”‚ (catalog) â”‚                       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Service Responsibilities

| Service | Language | Role | Lifecycle |
|---------|----------|------|-----------|
| **ratd** | Go | API server, auth, scheduling, plugin host, catalog ops, executor orchestrator | Long-running |
| **ratq** | Python | Interactive DuckDB queries (read-only), schema introspection | Long-running sidecar |
| **runner** | Python | Pipeline execution, DuckDB writes, PyArrow, Iceberg, quality tests | Long-running (warm pool) |
| **portal** | Next.js | Web IDE â€” editor, query, pipelines, DAG, explorer. THE user interface | Long-running |
| **postgres** | â€” | Platform state: runs, schedules, ownership, plugin config | Long-running |
| **minio** | â€” | S3-compatible object storage | Long-running |
| **nessie** | â€” | Iceberg REST catalog with git-like branching | Long-running |

**7 containers total for Community**. One `docker compose up` starts everything.

### Communication

- **Portal â†’ ratd**: REST API (HTTP)
- **ratd â†’ ratq**: gRPC (query dispatch, schema introspection)
- **ratd â†’ runner**: gRPC (pipeline submission, log streaming)
- **ratd â†’ plugins**: gRPC (auth, sharing, executor, audit)
- **ratd â†’ postgres**: SQL (platform state)
- **ratd â†’ minio**: S3 API (file operations)
- **ratd â†’ nessie**: Iceberg REST API (catalog operations)

---

## 1. Editions

### Community (Free, Open Source, Single-User)

Full-featured single-user data platform. Zero friction.

- Full ingestion flow (bronze layer, CSV/API/CDC sources)
- Full transform pipeline (silver/gold, SQL + Python)
- Full quality testing framework
- Full code execution (pipeline runs, scheduling, triggers)
- Full web IDE (editor, query, pipelines, DAG, explorer)
- Full query engine (DuckDB + Iceberg)
- S3 file management (MinIO)
- Namespace-based flat catalog â€” no limit on tables/pipelines
- No login required â€” open access, single user
- Plugin system â€” extensible by anyone
- `docker compose up` and go

### Pro (Paid, Closed-Source Container Plugins)

Everything in Community, plus:

- Authentication (OIDC / Keycloak / Cognito)
- Multi-user with roles (data engineer, analyst, maintainer)
- Multi-namespace (organizational boundaries)
- Per-object ownership + sharing (any owner shares anything)
- Projects (soft grouping for organization)
- Data catalog / discovery (search all accessible tables)
- Row-Level Security (auto-generated from sharing grants)
- Container-per-run execution isolation
- Cloud profiles (AWS / GCP deployment)
- Landing zones (shared inbox for data maintainers)
- Audit logging
- Support

---

## 2. Plugin Architecture

### Plugin = Container

Each plugin is a container image, not a Python package. The Go platform
communicates with plugins via gRPC.

```yaml
# rat.yaml
plugins:
  auth:
    image: ghcr.io/rat/plugin-auth-keycloak:latest
    config:
      issuer_url: http://keycloak:8180/realms/rat

  executor:
    image: ghcr.io/rat/plugin-executor-container:latest
    config:
      runtime: podman

  portal-pro:
    image: ghcr.io/rat/portal-pro:latest

  # Community default: no plugins = no auth, local executor, base portal
```

### Extension Points

| Slot | Community Default | Pro Plugin Examples |
|------|-------------------|--------------------|
| **Auth** | No-op (open access) | Keycloak OIDC (first), Cognito, Auth0 |
| **Enforcement** | Allow-all (single user) | API-level (ratd checks ACLs), S3 IAM (STS per-user for AWS) |
| **Sharing** | Allow-all (single user) | Per-object ownership, ACLs, projects |
| **RLS** | None | Row-Level Security (auto-generated from sharing grants) |
| **Executor** | WarmPoolExecutor (single warm runner) | ContainerExecutor (on-demand), LambdaExecutor, K8sJobExecutor |
| **Storage** | MinIO (S3 API) | AWS S3 native, GCS adapter |
| **Catalog** | Nessie | AWS Glue, BigLake |
| **Portal** | Base web IDE | Pro portal (extends base â€” sharing dialog, admin, audit viewer) |
| **Ingestion** | File upload, basic API | CDC connectors, streaming, bulk loaders |
| **Audit** | None | Audit logger, compliance reports |

### Plugin Interface (gRPC)

```protobuf
service AuthPlugin {
  rpc Authenticate(AuthRequest) returns (AuthResponse);
  rpc Authorize(AuthzRequest) returns (AuthzResponse);
}

service ExecutorPlugin {
  rpc Submit(PipelineSpec) returns (RunHandle);
  rpc Status(RunHandle) returns (RunStatus);
  rpc Logs(RunHandle) returns (stream LogLine);
  rpc Cancel(RunHandle) returns (CancelResponse);
}

service SharingPlugin {
  rpc Share(ShareRequest) returns (ShareResponse);
  rpc Revoke(RevokeRequest) returns (RevokeResponse);
  rpc ListAccess(AccessQuery) returns (AccessList);
  rpc Transfer(TransferRequest) returns (TransferResponse);
}

service EnforcementPlugin {
  rpc CanAccess(AccessRequest) returns (AccessResponse);
  rpc GetCredentials(CredentialRequest) returns (S3Credentials);  // for S3 IAM mode
}
```

### Portal Plugin Model

Pro portal is a **separate container image built FROM the community portal base**.

```dockerfile
# portal-pro/Dockerfile
FROM ghcr.io/rat/portal:latest
COPY pro-pages/ /app/src/app/(pro)/
COPY pro-components/ /app/src/components/pro/
RUN npm run build
```

- Community portal is the full open-source web IDE
- Pro portal image extends it with Pro pages (sharing dialog, admin dashboard, audit viewer)
- `ratd` exposes `GET /api/v1/features` â†’ portal shows/hides nav items based on active plugins
- Pro UI code never touches the public repo

### Plugin Discovery & Distribution

- Plugins declared in `rat.yaml`
- Platform starts plugin containers as sidecars (or connects to existing)
- Health check on startup, graceful degradation if plugin unavailable
- Community with zero plugins = fully functional, zero overhead
- **Distribution**: ghcr.io private packages (token-gated). Local registry (`registry:2`) for dev.

---

## 3. Repository Structure

Community monorepo + separate Pro repo. Shared local dev folder.

```
~/rat/                                  # umbrella folder (not a repo)
â”‚
â”œâ”€â”€ rat/                                # PUBLIC repo â€” community (monorepo)
â”‚   â”œâ”€â”€ platform/                       # Go â€” the brain
â”‚   â”‚   â”œâ”€â”€ cmd/ratd/                   # Single binary: API + scheduler + plugin host
â”‚   â”‚   â”œâ”€â”€ internal/
â”‚   â”‚   â”‚   â”œâ”€â”€ api/                    # HTTP API (chi or echo router)
â”‚   â”‚   â”‚   â”œâ”€â”€ auth/                   # Auth middleware + plugin slot
â”‚   â”‚   â”‚   â”œâ”€â”€ executor/              # Dispatches runs to executor plugin or local
â”‚   â”‚   â”‚   â”œâ”€â”€ scheduler/             # Cron / trigger engine
â”‚   â”‚   â”‚   â”œâ”€â”€ plugins/               # Plugin loader + gRPC clients
â”‚   â”‚   â”‚   â”œâ”€â”€ catalog/               # Nessie / Iceberg REST client
â”‚   â”‚   â”‚   â”œâ”€â”€ ownership/             # Per-object ownership + sharing registry
â”‚   â”‚   â”‚   â””â”€â”€ storage/               # S3 operations (MinIO Go SDK)
â”‚   â”‚   â”œâ”€â”€ go.mod
â”‚   â”‚   â””â”€â”€ Dockerfile                 # ~20MB final image (scratch + binary)
â”‚   â”‚
â”‚   â”œâ”€â”€ runner/                         # Python â€” the muscle
â”‚   â”‚   â”œâ”€â”€ src/rat_runner/
â”‚   â”‚   â”‚   â”œâ”€â”€ __main__.py            # Entrypoint (sys.path + serve)
â”‚   â”‚   â”‚   â”œâ”€â”€ server.py              # gRPC RunnerServiceImpl (6 RPCs) + cleanup daemon
â”‚   â”‚   â”‚   â”œâ”€â”€ executor.py            # 5-phase pipeline execution (branch â†’ write â†’ test â†’ merge)
â”‚   â”‚   â”‚   â”œâ”€â”€ engine.py              # DuckDB engine (S3 + Iceberg exts)
â”‚   â”‚   â”‚   â”œâ”€â”€ templating.py          # Jinja SQL â€” ref(), this, is_incremental(), watermark
â”‚   â”‚   â”‚   â”œâ”€â”€ iceberg.py             # PyIceberg writes (overwrite + merge) + watermark reads
â”‚   â”‚   â”‚   â”œâ”€â”€ nessie.py              # Nessie v2 REST client (branch lifecycle)
â”‚   â”‚   â”‚   â”œâ”€â”€ python_exec.py         # Python pipeline execution via exec()
â”‚   â”‚   â”‚   â”œâ”€â”€ quality.py             # Quality test discovery + execution
â”‚   â”‚   â”‚   â”œâ”€â”€ config.py              # S3Config, NessieConfig, YAML parsing, S3 listing
â”‚   â”‚   â”‚   â”œâ”€â”€ models.py              # RunState, RunStatus, LogRecord, QualityTestResult
â”‚   â”‚   â”‚   â”œâ”€â”€ log.py                 # RunLogger (deque + Python logging)
â”‚   â”‚   â”‚   â””â”€â”€ gen/                   # Generated gRPC stubs
â”‚   â”‚   â”œâ”€â”€ tests/unit/                # 140 tests, 91% coverage
â”‚   â”‚   â”œâ”€â”€ pyproject.toml
â”‚   â”‚   â””â”€â”€ Dockerfile                 # ~200MB (Python + DuckDB + PyArrow)
â”‚   â”‚
â”‚   â”œâ”€â”€ query/                          # Python â€” the reader
â”‚   â”‚   â”œâ”€â”€ src/rat_query/
â”‚   â”‚   â”‚   â”œâ”€â”€ __main__.py            # Entrypoint (sys.path + serve)
â”‚   â”‚   â”‚   â”œâ”€â”€ server.py              # gRPC QueryServiceImpl (4 RPCs) + shutdown
â”‚   â”‚   â”‚   â”œâ”€â”€ engine.py              # Long-lived DuckDB engine (S3 + DDL lock)
â”‚   â”‚   â”‚   â”œâ”€â”€ catalog.py             # NessieCatalog â€” discovery, view registration, 30s refresh
â”‚   â”‚   â”‚   â”œâ”€â”€ arrow_ipc.py           # Arrow IPC serialization helpers
â”‚   â”‚   â”‚   â”œâ”€â”€ config.py              # S3Config, NessieConfig (reuses runner pattern)
â”‚   â”‚   â”‚   â””â”€â”€ gen/                   # Generated gRPC stubs
â”‚   â”‚   â”œâ”€â”€ tests/unit/                # 52 tests, 87% coverage
â”‚   â”‚   â”œâ”€â”€ pyproject.toml
â”‚   â”‚   â””â”€â”€ Dockerfile
â”‚   â”‚
â”‚   â”œâ”€â”€ portal/                         # Next.js â€” the face (web IDE)
â”‚   â”‚   â””â”€â”€ ...
â”‚   â”‚
â”‚   â”œâ”€â”€ sdk-typescript/                 # TS SDK (for portal)
â”‚   â”‚
â”‚   â”œâ”€â”€ proto/                          # Shared gRPC protobuf definitions
â”‚   â”‚   â”œâ”€â”€ common/v1/common.proto      # Shared types (Layer, RunStatus, GetRunStatus, etc.)
â”‚   â”‚   â”œâ”€â”€ runner/v1/runner.proto      # Runner service (6 RPCs)
â”‚   â”‚   â”œâ”€â”€ query/v1/query.proto        # Query service (4 RPCs)
â”‚   â”‚   â”œâ”€â”€ executor/v1/executor.proto  # Executor plugin service
â”‚   â”‚   â”œâ”€â”€ auth/v1/auth.proto          # Auth plugin
â”‚   â”‚   â”œâ”€â”€ cloud/v1/cloud.proto        # Cloud plugin
â”‚   â”‚   â”œâ”€â”€ plugin/v1/plugin.proto      # Base plugin interface
â”‚   â”‚   â”œâ”€â”€ sharing/v1/sharing.proto    # Sharing plugin
â”‚   â”‚   â””â”€â”€ enforcement/v1/enforcement.proto
â”‚   â”‚
â”‚   â”œâ”€â”€ infra/                          # Docker compose, configs
â”‚   â”‚   â”œâ”€â”€ docker-compose.yml          # Community: all 7 services
â”‚   â”‚   â””â”€â”€ docker-compose.pro.yml      # Pro: override with plugin containers
â”‚   â”‚
â”‚   â””â”€â”€ docs/
â”‚
â””â”€â”€ rat-pro/                            # PRIVATE repo â€” Pro plugins only
    â”œâ”€â”€ plugins/
    â”‚   â”œâ”€â”€ auth-keycloak/              # Go container
    â”‚   â”œâ”€â”€ executor-container/         # Go container
    â”‚   â”œâ”€â”€ sharing/                    # Go container â€” ownership, ACLs, projects
    â”‚   â””â”€â”€ cloud-aws/                  # Go container
    â”œâ”€â”€ portal-pro/                     # Pro portal extension (FROM community portal)
    â””â”€â”€ infra/
        â””â”€â”€ docker-compose.pro.yml      # Plugin containers compose
```

**Removed from v1**: `cli/`, `sdk-python/` (no local dev, no CLI â€” everything through web IDE).

---

## 4. Why Go

| Factor | Go | Python (current) |
|--------|----|--------------------|
| Binary size | ~10-15MB | ~200MB+ with deps |
| Memory baseline | ~20MB | ~100MB+ |
| Startup | Instant | Seconds |
| Concurrency | Goroutines (native) | asyncio (fragile) |
| Container ecosystem | Native (Docker SDK, K8s client-go, MinIO SDK) | Wrappers |
| Deployment | Single static binary | virtualenv + deps |
| Long-running server | Built for this | GIL, memory leaks |

Python stays for what it does best: data manipulation, DuckDB, PyArrow, Iceberg writes.

---

## 5. Execution Model

### Runner Contract

The Go platform dispatches pipeline runs to the Python runner via gRPC:

```
ratd (Go) â”€â”€â”€â”€ gRPC â”€â”€â”€â”€â–¶ rat-runner (Python container)
                              â”‚
                              â”œâ”€â”€ Receives: pipeline spec, S3 creds, config
                              â”œâ”€â”€ Executes: DuckDB SQL or Python script
                              â”œâ”€â”€ Writes: results to S3/Iceberg
                              â”œâ”€â”€ Streams: logs + progress back via gRPC
                              â””â”€â”€ Long-running: warm pool, 4 concurrent runs
```

### Query Contract

Interactive queries go through the long-running query sidecar:

```
portal â”€â”€ REST â”€â”€â–¶ ratd (Go) â”€â”€ gRPC â”€â”€â–¶ ratq (Python)
                                             â”‚
                                             â”œâ”€â”€ DuckDB read-only queries
                                             â”œâ”€â”€ Schema introspection
                                             â”œâ”€â”€ Table preview / stats
                                             â””â”€â”€ Long-lived, connection pooled
```

### Executor Implementations

Every executor is a plugin. Community ships with WarmPoolExecutor. Pro swaps in others.

| Executor | How | Startup | Edition |
|----------|-----|---------|---------|
| WarmPoolExecutor | Single pre-started runner sidecar, picks up jobs from queue | ~0 sec (warm) | Community default |
| ContainerExecutor | Fresh container per run (podman/docker), full isolation | ~3-5 sec (cold) | Pro plugin |
| LambdaExecutor | ratd invokes Lambda with runner image | ~1-2 sec (warm) | Pro plugin |
| K8sJobExecutor | ratd submits K8s Job manifest | ~5-10 sec | Pro plugin |

### Executor Interface (Implemented)

Defined in `platform/internal/api/executor.go`:

```go
type Executor interface {
    Submit(ctx context.Context, run *domain.Run, pipeline *domain.Pipeline) error
    Cancel(ctx context.Context, runID string) error
}
```

**WarmPoolExecutor** (`platform/internal/executor/warmpool.go`):
- Connects to runner via ConnectRPC (`RunnerServiceClient`)
- `Submit()`: sends `SubmitPipelineRequest`, updates run to `running` or `failed`
- `Cancel()`: sends `CancelRunRequest`, removes from active map
- Background goroutine polls `GetRunStatus` every 5s for active runs
- Terminal status (`success`/`failed`) â†’ updates DB + removes from active map
- Wired in `main.go` when `RUNNER_ADDR` env var is set

### Runner Implementation (v2.1)

- Base: `python:3.12-slim` + DuckDB + PyArrow + PyIceberg + gRPC
- Long-running gRPC server on port 50052 (sync `grpcio`, not async)
- **4 pipeline workers** (ThreadPoolExecutor) + **10 gRPC server workers**
- One DuckDB connection per run (connections aren't thread-safe)
- In-memory run registry (`dict[str, RunState]`) â€” ratd owns truth in Postgres
- Bounded log deque (10K entries/run) for StreamLogs + Python logging for stdout
- Cancellation via `threading.Event` (cooperative, checked between phases)
- **Run cleanup daemon** â€” evicts terminal runs past `RUN_TTL_SECONDS` (default 3600)
- **Per-run STS credentials** â€” `s3_config` map in SubmitPipelineRequest for multi-tenant isolation
- **Ephemeral Nessie branches** â€” each run writes to `run-{run_id}`, merged to main on success
- **Pipeline types**: SQL (Jinja + DuckDB) and Python (`exec()` with injected globals)
- **Write modes**: 6 merge strategies â€” full refresh (overwrite), incremental (ANTI JOIN dedup), append only, delete+insert, SCD Type 2 (history tracking), snapshot (partition-aware). See ADR-014
- **Quality tests**: discovers `tests/quality/*.sql` from S3, 0 violation rows = pass, error severity blocks merge
- Entrypoint: `python -m rat_runner`
- See `docs/adr/005-runner-service.md` for full design rationale

### Scheduler (Implemented)

The scheduler runs as a background goroutine inside ratd (`platform/internal/scheduler/`).

- **Check interval**: 30 seconds (configurable via constructor)
- **Cron parser**: `robfig/cron/v3` (5-field: minute, hour, dom, month, dow)
- **Starts when**: both `DATABASE_URL` and `RUNNER_ADDR` are set (needs stores + executor)

**Tick cycle**:
1. Load all schedules from Postgres
2. Skip disabled schedules and invalid cron expressions
3. For schedules with nil `next_run_at`: compute it, persist, don't fire (first-time setup)
4. For schedules where `next_run_at <= now()`: create run, submit to executor, advance `next_run_at`

**Missed schedule policy**: Catch up once, then advance to future. If ratd was down for 3 hours
and a schedule was due, it fires exactly one run and sets `next_run_at` to the next future occurrence.
No backfill storm.

### Auth Middleware (Implemented)

Auth is a plugin slot on the `Server` struct:

```go
Auth func(http.Handler) http.Handler
```

- **Community**: `auth.Noop()` â€” passes every request through unchanged
- **Pro**: replace with real OIDC middleware from auth plugin container
- **Health endpoint** (`GET /health`): always unauthenticated (outside `/api/v1`)
- Applied via `r.Use(srv.Auth)` in the `/api/v1` route group

---

## 6. Ownership & Sharing Model (Pro Plugin)

### Per-Object Ownership

Every table and pipeline has an **owner** (the user who created it). Ownership is transferable.

```
ecommerce.bronze.raw_orders    owner: alice
ecommerce.silver.orders        owner: alice
ecommerce.gold.revenue         owner: alice
marketing.gold.attribution     owner: bob
```

### Sharing

Owners grant access per-object. Cross-namespace sharing is natural.

```
SHARE ecommerce.silver.orders WITH bob READ
SHARE ecommerce.gold.revenue WITH charlie READ
SHARE ecommerce.bronze.* WITH role:data-engineer WRITE
TRANSFER marketing.gold.attribution TO alice
```

### Projects (Soft Grouping)

Projects are labels, not containers. A table can belong to multiple projects.
No isolation â€” just organization for humans.

```
PROJECT ecommerce: [ecommerce.bronze.raw_orders, ecommerce.silver.orders, ecommerce.gold.revenue]
PROJECT marketing: [marketing.bronze.campaigns, marketing.gold.attribution, ecommerce.gold.revenue]
                                                                              â†‘ shared across projects
```

### Access Resolution

1. Owner â†’ full access (read, write, delete, share)
2. Explicit share â†’ granted access level (read or write)
3. Role-based â†’ `role:data-engineer` gets access to everything shared with that role
4. Namespace membership â†’ members can discover (not access) all objects in their namespace
5. Community â†’ no checks, single user owns everything

---

## 7. Web IDE (Portal)

The portal is the **only** user interface. No CLI, no local dev.

### Core Features (Community)

- **Code Editor**: CodeMirror 6 + SQL/Python syntax + autocomplete + `ref()` resolution
- **Query Console**: Interactive DuckDB queries with result grid
- **Pipeline Manager**: Create, configure, run, schedule pipelines
- **DAG Viewer**: Full lineage graph (Mermaid)
- **Data Explorer**: Browse tables, preview data, view schema
- **File Browser**: S3-backed tree view, create/edit/delete files
- **Quality Dashboard**: Test results, trends, severity badges
- **Run History**: Timeline of pipeline runs, logs, metrics
- **Markdown Docs**: README.md rendering + in-place editing

### Pro Portal Extension

Built FROM community base image, adds:

- **Sharing Dialog**: Share tables/pipelines with users, set permissions
- **Admin Dashboard**: User management, namespace management, resource usage
- **Audit Viewer**: Who did what, when, compliance trail
- **Namespace Switcher**: Switch between namespaces in nav

### Built-in CI/CD

No external CI/CD needed. The platform handles the full lifecycle:

```
Write code (editor) â†’ Run pipeline â†’ Quality tests auto-run
       â†“                    â†“                  â†“
  Save to S3        Results to Iceberg    Results to .meta.yaml
                                                  â†“
                                          Dashboard updates
```

- **Schedules**: Cron-based triggers, configured in UI
- **Sensors**: Watch for upstream changes, auto-trigger downstream
- **Promote**: Run in bronze â†’ test â†’ promote to silver (all in UI)
- **Rollback**: One-click rollback via Iceberg time-travel

---

## 8. Cloud Compatibility (Pro Plugins)

| Component     | Community (local)  | Pro Plugin (aws)   | Pro Plugin (gcp) |
|---------------|--------------------|--------------------|------------------|
| Storage       | MinIO              | S3 native          | GCS              |
| Catalog       | Nessie             | Glue / Nessie      | BigLake          |
| Compute       | LocalExecutor      | Lambda / ECS       | Cloud Run        |
| Auth          | No-op              | Cognito            | Cloud IAM        |
| Orchestration | Built-in scheduler | Step Functions     | Cloud Composer   |
| State DB      | Postgres           | RDS Postgres       | Cloud SQL        |

---

## 9. Sequencing

| Phase | Focus |
|-------|-------|
| **v1 (now)** | Ship working Python product, learn from it |
| **v2.0** | Go platform (`ratd`) â€” API âœ…, auth middleware âœ…, executor âœ…, scheduler âœ…, plugin host |
| **v2.1** | Python runner container + gRPC contract âœ… (140 tests, 91% coverage). Includes: incremental pipelines, Python pipelines, quality tests, ephemeral Nessie branches, run cleanup, per-run STS. See ADR-005 |
| **v2.2** | Python query service (`ratq`) + gRPC contract âœ… (52 tests, 87% coverage). Includes: long-lived DuckDB, Nessie catalog discovery, 30s background refresh, Arrow IPC serialization, Go ConnectRPC client. See ADR-006 |
| **v2.3** | Portal rewrite â€” full web IDE connected to Go API âœ… |
| **v2.4** | Plugin system foundation âœ… â€” `rat.yaml` config, proto definitions (auth, sharing, enforcement, plugin), plugin loader with health checks, auth middleware delegation, dynamic features. 31 new tests. See ADR-007 |
| **v2.5** | First Pro plugin: auth-keycloak âœ… â€” Go container (~26MB) validates Keycloak JWTs via ConnectRPC. JWKS cache with rate-limited refresh, OIDC discovery, background prefetch with retry. Claim mapping: subâ†’user_id, email, nameâ†’display_name, policyâ†’roles. Authorize stub (CodeUnimplemented until v2.7). Docker Compose overlay pattern established. Adapted v1 realm JSON. 17 tests. See ADR-008 |
| **v2.6** | ContainerExecutor Pro plugin âœ… â€” per-run container isolation via Podman API. Runner single-shot mode (`RUN_MODE=single`), raw HTTP to Podman socket (~25MB image), PluginExecutor adapter in community, reaper for container cleanup. Warm runner disabled in Pro (`replicas: 0`). ~40 new tests. See ADR-009 |
| **v2.7** | ACL plugin: per-object ownership + sharing + enforcement. Authorizer abstraction in community (NoopAuthorizer), PluginAuthorizer in Pro. SQLite-backed ACL store. REST sharing endpoints. Permission hierarchy (admin > write > read). ~55 new tests. See ADR-010 |
| **v2.8** | Cloud plugin (AWS first) âœ… â€” STS credential vending (per-namespace scoped S3 creds via AssumeRole + inline policy), ECS Fargate executor (Submit/GetRunStatus/StreamLogs/Cancel), CloudWatch log streaming. New `cloud/v1/cloud.proto` with `CloudService.GetCredentials`. Runner STS support (session_token in S3Config, DuckDB, PyIceberg, boto3). Cloud plugin slot in ratd loader. ~33 plugin tests. See ADR-011 |
| **v2.9** | License gating + plugin distribution âœ… â€” RSA-256 signed JWT license keys, offline validation (no phone-home). Per-plugin entitlements via `plugins[]` claim. Single `RAT_LICENSE_KEY` env var. Plugins validate signature at startup, return `STATUS_NOT_SERVING` if unlicensed. ratd decodes JWT for display (no crypto deps in community). `pkg/license` shared Go module. `rat-license` CLI for key generation. Portal settings page with license status + expiry warning banner. ghcr.io distribution via GitHub Actions. ~10 license lib tests, ~3 CLI tests, ~4 handler tests per plugin, ~5 ratd decode tests. See ADR-012 |
| **v2.10** | Landing zones âœ… â€” standalone file drop areas for raw data uploads. Postgres-tracked metadata (zone name, namespace, file registry). S3 path: `{namespace}/landing/{zoneName}/{filename}`. 8 REST endpoints (`/api/v1/landing-zones`). DuckDB preview via existing `/query` endpoint (`read_csv_auto`, `read_json_auto`, `read_parquet`). Multipart upload (32MB max). TS SDK `LandingResource` (8 methods). Portal: zone list page, zone detail with drag-and-drop upload, file preview modal. ~14 handler tests, ~8 store tests. See ADR-013 |
| **v2.11** | Merge strategies âœ… â€” 6 write strategies (full_refresh, incremental, append_only, delete_insert, scd2, snapshot). Config merge: config.yaml base + annotation overrides per-field. 4 new Iceberg write functions (append, delete+insert, SCD2 history, partition snapshot). 4 new Jinja helpers (is_append_only, is_delete_insert, is_scd2, is_snapshot). Portal merge strategy settings card. SDK MergeStrategy + PipelineConfig types. ~30 new runner tests, full-stack typecheck. See ADR-014 |

---

## 10. Resolved Decisions

| # | Question | Decision |
|---|----------|----------|
| 1 | Plugin communication | **gRPC** via **ConnectRPC** (gRPC-compatible + HTTP/1.1 friendly) |
| 2 | Plugin distribution | **ghcr.io private** (token-gated) + **local registry** (`registry:2`) for dev |
| 3 | Portal plugins | **Separate container** built FROM community portal base |
| 4 | Runner â†” ratd streaming | **gRPC streaming** (logs + progress) |
| 5 | Service split | **Max split** â€” 7 containers, one `docker compose up` |
| 6 | Portal embedded? | **No** â€” separate container, independent release cycle |
| 7 | Platform state DB | **Postgres** |
| 8 | CLI | **Removed** â€” all interaction through web IDE. REST API remains for programmatic access |
| 9 | v1 backward compat | **None** â€” fresh start, clean v2 |
| 10 | Naming collisions | **Namespaces** â€” `namespace.layer.table` (Community: implicit `default` ns) |
| 11 | Repo structure | **Community monorepo** + **Pro separate private repo** |
| 12 | Enforcement model | **Plugin slot** â€” API-level (Community/local) or S3 IAM (AWS). Auth plugin is separate (OIDC) |
| 13 | Auth first impl | **Keycloak (OIDC)** â€” standard protocol, swap to Cognito by changing `issuer_url` |
| 14 | Nessie branches | **Flat main branch** for production. Ephemeral per-run branches for write isolation + quality gating. Dev branches for experimentation |
| 15 | RLS | **Plugin, later** â€” not in v2.0. Sharing = full table access or nothing |
| 16 | Community executor | **WarmPoolExecutor** â€” single pre-started runner sidecar, ~0 sec startup |
| 17 | Pro executor | **On-demand** â€” fresh container per run (ContainerExecutor), fully pluggable |
| 18 | Service discovery | **Compose DNS** â€” `ratq:50051`, no service registry needed |
| 19 | Proto tooling | **buf.build** (linting, breaking change detection) + **ConnectRPC** framework |
| 20 | Feature flags | **`GET /api/v1/features`** â€” JSON with plugin status, edition, capabilities |
| 21 | Namespace creation | **Admin-only** â€” Community gets `default` auto-created |
| 22 | Data retention | **100 runs/pipeline** in Postgres, **10 in .meta.yaml**, **30d logs** in S3, all configurable |
| 23 | Postgres schema | **Done** â€” 14 tables defined in `docs/postgres-schema.sql` (12 original + landing_zones + landing_files) |
| 24 | S3 StorageStore | **MinIO Go SDK** (`minio-go/v7`) â€” `internal/storage/S3Store` implements `api.StorageStore`. Bucket auto-created on startup. 8 integration tests. See `docs/adr/001-s3-storage.md` |
| 25 | File upload flow | **Through ratd** â€” multipart POST `/api/v1/files/upload` (32MB max). No presigned URLs for Community. |
| 26 | Auth middleware | **Plugin slot** â€” `func(http.Handler) http.Handler` field on Server. Community: `auth.Noop()` (pass-through). Pro: swappable via plugin. See `docs/adr/002-auth-middleware.md` |
| 27 | Community executor | **WarmPoolExecutor** â€” dispatches to single warm runner via ConnectRPC. Polls status every 5s. See `docs/adr/003-warmpool-executor.md` |
| 28 | Scheduler | **Background goroutine** â€” 30s ticker, `robfig/cron/v3` for parsing. Missed schedules catch up once, then advance. See `docs/adr/004-scheduler.md` |
| 29 | Executor interface | **In `api/` package** â€” `Submit(ctx, run, pipeline) error` + `Cancel(ctx, runID) error`. Avoids import cycle. |
| 30 | Run dispatch | **Best-effort** â€” run is persisted in DB before executor dispatch. Executor failure doesn't lose the run. |
| 31 | Runner architecture | **Sync gRPC + ThreadPoolExecutor** â€” 4 pipeline workers, 10 gRPC workers. One DuckDB per run, in-memory state, bounded log deques. Cleanup daemon evicts terminal runs past TTL. See `docs/adr/005-runner-service.md` |
| 32 | Runner config | **Env vars** â€” S3_ENDPOINT, S3_ACCESS_KEY, S3_SECRET_KEY, S3_BUCKET, S3_USE_SSL, NESSIE_URL, GRPC_PORT, RUN_TTL_SECONDS. Per-run STS via `s3_config` map for Pro. |
| 33 | ref() resolution | **Parquet glob** for MVP â€” `read_parquet('s3://bucket/ns/layer/name/**/*.parquet')`. Future: `iceberg_scan()` once metadata paths standardized. |
| 34 | Pipeline types | **SQL + Python** â€” SQL via Jinja/DuckDB, Python via `exec()` with injected globals. `.py` checked first, then `.sql`. |
| 35 | Write modes | **6 merge strategies** â€” full_refresh, incremental, append_only, delete_insert, scd2, snapshot. Config merge: config.yaml base + annotation overrides per-field. See ADR-014 |
| 36 | Quality tests | **Post-write SQL tests** â€” discover from S3 `tests/quality/*.sql`, compile via Jinja, execute, 0 rows = pass. `-- @severity: error` blocks merge. |
| 37 | Nessie branches | **Ephemeral per-run** â€” `run-{run_id}` branch created, writes happen there, merged to main on success, deleted on quality failure. |
| 38 | ratq architecture | **Long-lived DuckDB sidecar** â€” single connection, Nessie v2 REST for table discovery, 30s background refresh, Arrow IPC serialization. DDL lock for view registration only. 52 tests, 87% coverage. See `docs/adr/006-query-service.md` |
| 39 | ratq table discovery | **Nessie REST API** â€” `GET /api/v2/trees/main/entries`, filter for ICEBERG_TABLE. Authoritative (no S3 listing). Tables registered as DuckDB views via `read_parquet()` glob. |
| 40 | Arrow IPC deserialization (Go) | **Apache Arrow Go** (`apache/arrow-go/v18`) â€” ratd deserializes Arrow IPC bytes from ratq into `[]map[string]interface{}` for JSON REST responses. Zero-copy friendly, type-preserving. |
| 41 | Plugin config format | **`rat.yaml`** â€” YAML with `edition` + `plugins` map. `RAT_CONFIG` env var > `./rat.yaml` > community defaults (no file needed). `gopkg.in/yaml.v3`. See ADR-007 |
| 42 | Plugin lifecycle | **ratd connects, never starts containers.** Compose/K8s manages lifecycle. `addr` field only (not `image`). See ADR-007 |
| 43 | Plugin health check | **Custom `PluginService.HealthCheck` RPC** â€” every plugin implements it. 5s timeout. Unhealthy â†’ disabled with warning, ratd continues. See ADR-007 |
| 44 | Auth delegation | **Bearer token â†’ `AuthService.Authenticate` RPC** â€” middleware extracts token, calls plugin, stores `UserIdentity` in context. Falls back to `auth.Noop()` when no plugin. See ADR-007 |
| 45 | Dynamic features | **`PluginRegistry` interface** â€” `Registry.Features()` returns actual plugin status. Portal uses `GET /api/v1/features` for dynamic UI. See ADR-007 |
| 46 | Pro executor | **ContainerExecutor** â€” fresh container per run via Podman API. Raw HTTP over Unix socket, no external deps. `RUNNER_IMAGE` env var for runner image. See ADR-009 |
| 47 | Runner single-shot mode | **`RUN_MODE=single`** â€” runner reads env vars, calls `execute_pipeline()`, prints JSON result to stdout, exits 0/1. No gRPC overhead. See ADR-009 |
| 48 | Executor plugin protocol | **`ExecutorService` (ConnectRPC)** â€” `Submit`, `GetRunStatus`, `StreamLogs`, `Cancel`. `PluginExecutor` adapter in community mirrors `WarmPoolExecutor` pattern. See ADR-009 |
| 49 | Container resource limits | **CFS quota** â€” CPU cores Ã— period microseconds. Default 2.0 cores + 1GB memory. Configurable via env vars. |
| 50 | Container cleanup | **Reaper goroutine** â€” sweeps exited containers after TTL (default 10min). Orphan cleanup on startup (kill running + remove all with executor label). |
| 51 | Executor fallback | **Graceful degradation** â€” if executor plugin unhealthy, ratd falls back to WarmPoolExecutor. `Features()` reports "container" vs "warmpool". |
| 52 | Merge strategies | **6 strategies** â€” full_refresh (overwrite), incremental (ANTI JOIN dedup), append_only, delete_insert, scd2 (history tracking), snapshot (partition-aware). Config merge: config.yaml + annotations overlay. See ADR-014 |
| 53 | Config merge | **Annotations overlay config.yaml** â€” config.yaml is base, annotations override per-field. Enables both portal UI and power-user annotations. See ADR-014 |
| 54 | Template helpers | **Strategy-aware** â€” `is_incremental()`, `is_append_only()`, `is_delete_insert()`, `is_scd2()`, `is_snapshot()` Jinja functions for strategy-specific SQL. |

---

## 11. Open Questions (Remaining)

### Architecture

- [x] Runner warm pool: **4 concurrent workers** (ThreadPoolExecutor). Single runner container, not configurable yet. Pro swaps in ContainerExecutor for per-run isolation.
- [x] ratq connection pooling: **Single long-lived DuckDB connection**, DuckDB 1.0+ handles concurrent reads internally. `threading.Lock` for DDL only (view registration). 10 gRPC server workers. Memory limits not yet configurable.
- [x] Should ratd manage the runner lifecycle? **No** â€” compose `restart: unless-stopped` + gRPC healthcheck. ratd detects stale runs via polling.

### Proto & API Design

- [ ] Full proto message definitions (PipelineSpec, RunHandle, AccessRequest, etc.)
- [x] ratd REST API endpoint spec â€” done (`docs/api-spec.md`, 43 endpoints implemented)
- [x] ratq gRPC service definition â€” done (`proto/query/v1/query.proto`, 4 RPCs: ExecuteQuery, GetSchema, PreviewTable, ListTables). Arrow IPC for result serialization. See ADR-006
- [ ] Error handling strategy across gRPC services (status codes, error details)

### Data & Storage

- [x] Postgres schema design â€” done (`docs/postgres-schema.sql`, 12 tables)
- [ ] Migrations strategy (sqlc, goose, or manual?)
- [x] S3 metadata headers convention: **content-type auto-detected** by extension (.sql â†’ `application/sql`, .py â†’ `text/x-python`, .yaml â†’ `application/x-yaml`, etc.). File type classification (`pipeline-sql`, `config`, `test`, etc.) returned in `ListFiles` response.
- [x] Iceberg table naming convention: **`namespace.layer.table_name`** as Nessie table path (dot-separated)
- [x] How does runner resolve `ref("silver.orders")`? **Jinja template** â€” `ref('layer.name')` â†’ `read_parquet('s3://bucket/ns/layer/name/**/*.parquet')`. Cross-namespace: `ref('ns.layer.name')`. Future: Iceberg scan.

### Portal & UX

- [ ] Portal route map (which pages, navigation structure, which API calls per page)
- [ ] How does the portal handle real-time updates? (polling, WebSocket, SSE for run logs?)
- [ ] Editor experience: how does autocomplete resolve `ref()` calls? (ratq introspection?)
- [x] File upload flow: **through ratd** â€” multipart POST to `/api/v1/files/upload` (32MB max), ratd writes to S3. Simpler auth, no presigned URL complexity.

### DevOps & Tooling

- [ ] Docker compose file for 7 services (ports, volumes, health checks, dependencies)
- [ ] Dev workflow: how to develop platform + runner + portal simultaneously? (hot reload?)
- [ ] CI/CD for the community monorepo (GitHub Actions? What runs on PR?)
- [ ] Go module structure: single `go.mod` or per-package?

### Plugin System

- [x] Plugin lifecycle: **ratd connects, never starts containers.** Compose/K8s manages lifecycle. ratd connects to already-running gRPC endpoints. See ADR-007.
- [x] Plugin health check protocol: **Custom `PluginService.HealthCheck` RPC** â€” every plugin implements it. STATUS_SERVING = enabled, anything else = disabled with warning. 5s timeout. See ADR-007.
- [x] Plugin config validation: **Yes** â€” `rat.yaml` validated on load. Missing `addr` â†’ error. Unknown plugin names â†’ warning + skip. See `internal/config/`.
- [x] Can community users write their own plugins? **Yes** â€” proto definitions are in the public repo (`proto/plugin/v1/`, `proto/auth/v1/`, etc.). Any container implementing the gRPC interface can be loaded via `rat.yaml`.

---

## 12. What We Keep From v1

Not everything gets rewritten. These survive as-is or with minor adaptation:

- **Portal** (Next.js) â€” enhanced to be the full web IDE, talks to Go API
- **SDK-TypeScript** â€” for portal, updated for new API
- **Runner core logic** â€” DuckDB engine, PyArrow I/O, quality tests, SQL templating
- **Pipeline format** â€” `pipeline.sql`, `config.yaml`, `tests/` structure
- **S3 layout** â€” medallion layers, `.meta.yaml` sidecars (under namespace prefix)
- **Iceberg tables** â€” data is data, format doesn't change
- **UI theme** â€” underground/squat aesthetic carries forward

What gets rewritten in Go:
- API server (FastAPI â†’ Go chi/echo)
- Auth system (JWT/Keycloak middleware â†’ plugin slot)
- Ownership registry (replaces workspace manager)
- Pipeline discovery
- Run orchestration
- Scheduler/triggers

What gets removed:
- **CLI** (replaced by web IDE)
- **Python SDK** (no local dev)
- **Python API** (replaced by Go ratd)

---

*"Not everyone can become a great data engineer, but a great data platform can come from anywhere."* ğŸ€
